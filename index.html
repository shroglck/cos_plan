<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="CSCI Person ReID (ICCV)">
  <meta property="og:title" content="CSCI Person ReID (ICCV)"/>
  <meta property="og:description" content="Person CC-ReID using colors (ICCV)"/>
  <meta property="og:url" content="https://github.com/ppriyank/ICCV-CSCI-Person-ReID"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement">
  <meta name="twitter:description" content="Person CC-ReID using colors (ICCV)">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner.jpeg">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Person ReID biometrics survillence ICCV">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement</title>
  <link rel="icon" type="image/x-icon" href="static/images/banner.jpeg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css"> 
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css"> 
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script> 
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://ppriyank.github.io" target="_blank">Priyank Pathak</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=D_JvEcwAAAAJ&hl=en" target="_blank">Yogesh Singh Rawat</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Center for Research in Computer Vision, University of Central Florida<br>
                      <b style="color:red;">ICCV 2025 ü•≥</b>
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                        
                    
                      
                    <!-- Paper -->
                    <span class="link-block">
                      <a href="https://openaccess.thecvf.com/content/ICCV2025/html/Pathak_Colors_See_Colors_Ignore_Clothes_Changing_ReID_with_Color_Disentanglement_ICCV_2025_paper.html" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon"><i class="fas fa-file-pdf"></i></span>
                        <span>Publication</span>
                      </a>
                    </span>

                    <!-- Paper -->
                    <span class="link-block">
                      <a href="https://iccv.thecvf.com/virtual/2025/poster/2481" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon"><i class="fas fa-file-pdf"></i></span>
                        <span>Paper</span>
                      </a>
                    </span>
                      

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary (soon)</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ppriyank/ICCV-CSCI-Person-ReID" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                  </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2507.07230" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
                </span>

                <!-- SLIDES -->
                <span class="link-block">
                  <a href="https://iccv.thecvf.com/media/iccv-2025/Slides/2481.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-powerpoint"></i></span>
                    <span>Slides</span>
                  </a>
                </span>
                
                <!-- Poster -->
                <span class="link-block">
                  <a href="https://iccv.thecvf.com/media/PosterPDFs/ICCV%202025/2481.png?t=1755029754.2380598" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fas fa-file-image"></i></span>
                    <span>Poster</span>
                  </a>
                </span>

                <!-- Video -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=tWMQR6tEcww" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon"><i class="fab fa-youtube fa-lg"></i></span>
                     
                    <span>Video</span>
                  </a>
                </span>
                

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">


      <div style="display: flex; flex-wrap: wrap; justify-content: center; align-items: center; gap: 10px;">
        <img src="static/images/colors_Gif/vid 2.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 3.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 4.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 5.gif" width="11%" >
        <img src="static/images/colors_Gif/vid.gif" width="11%"  >
        <img src="static/images/colors_Gif/vid 6.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 7.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 8.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 9.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 10.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 11.gif" width="11%" >
        <img src="static/images/colors_Gif/vid 12.gif" width="11%">
        <img src="static/images/colors_Gif/vid 13.gif" width="11%">
        <img src="static/images/colors_Gif/vid 14.gif" width="11%">
        <img src="static/images/colors_Gif/vid 15.gif" width="11%">
        <img src="static/images/colors_Gif/vid 16.gif" width="11%">
        </div>
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4" type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        Clustering using color histograms groups images/video frames based on people wearing the same clothes. Video frames taken from the CCVID dataset. This suggests that colors can serve as a proxy for clothing labels.
      </h2>

    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract üòè</h2>
        <div class="content has-text-justified">
          <p>
            Clothes-Changing Re-Identification (CC-ReID) aims to recognize individuals across different locations and times, irrespective of clothing. 
            Existing methods often rely on additional models or annotations to learn robust, clothing-invariant features, making them resource-intensive. 
            In contrast, we explore the use of color‚Äîspecifically foreground and background colors‚Äîas a lightweight, annotation-free proxy for mitigating appearance bias in ReID models.
            We propose <b>Colors See, Colors Ignore (CSCI)</b>, a RGB-only method that leverages color information directly from raw images or video frames. CSCI efficiently captures color-related appearance bias (<b>'Color See'</b>) while disentangling it from identity-relevant ReID features (<b>'Color Ignore'</b>). To achieve this, we introduce <b>S2A self-attention</b>, a novel self-attention to prevent information leak between color and identity cues within the feature space. Our analysis shows a strong correspondence between learned color embeddings and clothing attributes, validating color as an effective proxy when explicit clothing labels are unavailable.
            We demonstrate the effectiveness of CSCI on both image and video ReID with extensive experiments on four CC-ReID datasets. 
            We improve baseline by Top-1 2.9% on LTCC and 5.0% on PRCC for image-based ReID, and 1.0% on CCVID and 2.5% on MeVID for video-based ReID without relying on additional supervision.  Our results highlight the potential of color as a cost-effective solution for addressing appearance bias in CC-ReID..
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- METHOD -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method üòé</h2>
        <img src="static/images/csci.gif" alt="MY ALT TEXT" width="80%"/>
        <div class="content has-text-justified">
          <p>
            Traditional transformer-based ReID models use RGB spatial pages of input and pass them through layers of transformers. The class token is used as <b style="color:rgb(13, 141, 163);">ReID Token</b> for inference and is trained using triplet loss, an identity-based classifier. 
            We introduce one additional class token, called <b style="color:rgb(255, 0, 0);">Color Token</b>, for which learns color embedding via MSE (regression) on color histograms. 
            We then disentangle Color Token from ReID token using cosine loss.
          </p>
        </div>
        <!-- <br><br><br><br> -->
        <div class="columns is-centered">
            <div class="column">
              <div class="content">
                <div class="image-container">
                  <img src="static/images/SA1.jpg" class="equal-image" />
                </div>
                <p class="carousel-caption">
                  <b>Traditional Self-Attention</b><br> shares information across all tokens, leaking information across ReID (biometrics) and Color tokens (appearance bias). This is an example of <b>100% overlap</b> between biometrics and appearance bias.
              </div>
            </div>

            <div class="column">
              <div class="content">
                <div class="image-container">
                  <img src="static/images/SA2.jpg" class="equal-image" />
                </div>
                <p class="carousel-caption">
                  <b>Masked Self-Attention</b><br> doesn't allow information sharing across ReID (biometrics) and Color tokens (appearance bias); however, ReID and Color tokens influence the weight of each other on spatial tokens. This is an example of <b>0% overlap</b> between biometrics and appearance bias, but they influence each other's weight.
              </div>
            </div>

            <div class="column">
              <div class="content">
                <div class="image-container">
                  <img src="static/images/SA3.jpg" class="equal-image" />
                </div>
                <p class="carousel-caption">
                  <b style="color:rgb(255, 0, 0);" >S2A Self-Attention (Ours ‚ò∫Ô∏è)  </b><br> By doing two-step self-attention, Color tokens (appearance bias) no longer influence the weight of the ReID tokens (biometrics) and vice-versa, "exactly like" Masked Self-Attention, and an example of <b>0% overlap</b> between biometrics and appearance bias. By adjusting the weights of the averaging of the spatial tokens; one aware of biometrics, and the other aware of appearance bias, we can influence which signal gets more weightage (biometrics or appearance bias). Current hyperpater is set to equal weight for both (1/2 weight)
              </div>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- -->




<!-- METHOD -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Alternatives üò±</h2>
        <div class="content has-text-justified">
          <p>
            ‚ñ∫ Alternative to our S2A self-attention would be to use two transformers, one for ReID and the other for appearance bias, which is computationally impractical for deployment, currently deployed by many ReID works as 2 ResNets or 2 transformers: one for biometrics and other for appearence bias (diffusion models for clothes, LLMs for clothes description). <br>
            ‚ñ∫ Another alternative to S2A self-attention would be to just leak the information between biometrics and appearence bias by sharing the backbone, most famously done by CAL CC-ReID. In Transformers, that would be "Traditional Self-Attention".  <br>
            ‚ñ∫ An alternative to using color would be to use "traditional" clothing integer annotations instead of colors. However, colors are more expressive than integer clothing labels.  <br>
            ‚ñ∫ Another alternative would be that LLM-based fine-grained clothing description; computationally infeasible. Fine-grained description needs to be generated per frame on video, as clothing may change across video <br>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Results üßê</h2>
      Numbers reported are an average of two runs, however <b class="blink"> pretrained weights are provided for the best performing models which have much higher accuracy than the reported values</b>.
       
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
         <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/generalization.png" width="60%" />
        </div>
        <h2 class="subtitle has-text-centered">
          <br><b>Generalization</b> <br>CSCI apporach generalizes across various previous works involving transformers and consistently outperforms traditional integer clothing labels.
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/img_result.png" width="100%" />
        </div>
        <h2 class="subtitle has-text-centered">
          <br><b>Image ReID Results</b> <br>CSCI doesn't require any annotation or supervision, unlike previous works. 
        </h2>
      </div>
      
      <div class="item">
        <!-- Your image here -->
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/video_result.png" width="100%" />
        </div>
        <h2 class="subtitle has-text-centered">
         <br><b>Video ReID Results</b> <br>CSCI doesn't require any annotation or supervision, unlike previous works. EZ-CLIP singnificantly improves Video ReID performance.
       </h2>
      </div>

      <div class="item">
        <!-- Your image here -->
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/self-attention.png" width="100%" />
        </div>
        <h2 class="subtitle has-text-centered">
         <br><b>Self-Attention alternatives</b> <br>S2A self-attention outperforms Masked and traditional self-attention, indicating the strong need for preventing 1) information leak between biometrics signals and appearance bias. 2) Preventing appearance bias influence on the weight of biometrics signals and vice versa. 
       </h2>
      </div>

      <div class="item">
        <!-- Your image here -->
        <div style="display: flex; justify-content: center; align-items: center;">
          <img src="static/images/color_variation.png" width="50%" />
        </div>
        <h2 class="subtitle has-text-centered">
         <br><b>Alternatives to clothes</b><br> Colors outperform tradtional integer clothing annotations and grey colored inputs.
       </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Ablation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Ablation ü•∏</h2>
        <div class="columns is-centered" style="gap: 2rem;">
            <div class="column">
              <div class="content">
                <div class="image-container2">
                  <img src="static/images/SCDK.png" class="equal-image2" />
                </div>
                <p class="carousel-caption">
                  Same clothes labels but different K means cluster of color embeddings indicates models take into account illumination, and other environmental factors. Depending on how you see it, it can be a <b>limitation</b> as "noisy clothing labels" or color embeddings indicate what's happening in the exact moment, aka <b>"Adaptive"</b>. 
                </p>
              </div>
            </div>

            <div class="column">
              <div class="content">
                <div class="image-container2">
                  <img src="static/images/DCSK.png" class="equal-image2" />
                </div>
                <p class="carousel-caption">
                  Different clothes but similar K-means cluster of color embeddings indicate the true  <b>limitation</b> of colors. Colors take into account overall frames, which may make models relate different images based on similar clothing foreground and background colors. Solution : horizontal / vertical splits of colors / fine-grained colors?
              </div>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- -->




<!--BibTex citation -->
  <section class="section is-light" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX ü•π</h2>
      <pre><code>
  @InProceedings{Pathak_2025_ICCV,
    author    = {Pathak, Priyank and Rawat, Yogesh S.},
    title     = {Colors See Colors Ignore: Clothes Changing ReID with Color Disentanglement},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2025},
    pages     = {16797-16807}
  }
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the<a href="https://nerfies.github.io" target="_blank">Nerfies</a>project page.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
